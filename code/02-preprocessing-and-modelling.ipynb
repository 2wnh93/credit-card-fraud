{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Modelling\n",
    "\n",
    "In this notebook, I scale the data and explore oversampling and undersampling techniques to balance the classes, all these prior to modelling. Subsequently, run data across classification algorithms - Logistic Regression, Random Forest Classifier, Support Vector Classifier and XGBoost Classifier.\n",
    "\n",
    "**Contents:**\n",
    "- [Imports](#Import-Libraries)\n",
    "- [Preprocessing](#Preprocessing)\n",
    "  - [Model Preparation](#Model-Prep)\n",
    "  - [Scaling Data](#Scaling-Data)\n",
    "  - [Balance Classes](#Balance-Classes)\n",
    "- [Model Selection](#Model-Selection)\n",
    "- [Model Evaluation](#Model-Evaluation)\n",
    "- [Summary](#Summary)\n",
    "- [Learning Points](#Learning-Points)\n",
    "- [Further Improvements](#Further-Improvements)\n",
    "- [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imported successfully!\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns',None)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "from prettytable import PrettyTable\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import ADASYN\n",
    "# modelling\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "print(\"All imported successfully!\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================== Top 5 rows of dataset ===================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>v10</th>\n",
       "      <th>v11</th>\n",
       "      <th>v12</th>\n",
       "      <th>v13</th>\n",
       "      <th>v14</th>\n",
       "      <th>v15</th>\n",
       "      <th>v16</th>\n",
       "      <th>v17</th>\n",
       "      <th>v18</th>\n",
       "      <th>v19</th>\n",
       "      <th>v20</th>\n",
       "      <th>v21</th>\n",
       "      <th>v22</th>\n",
       "      <th>v23</th>\n",
       "      <th>v24</th>\n",
       "      <th>v25</th>\n",
       "      <th>v26</th>\n",
       "      <th>v27</th>\n",
       "      <th>v28</th>\n",
       "      <th>amount</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.991390</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>1.468177</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>0.489095</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.635558</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>-0.183361</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>0.717293</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>2.345865</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>-0.121359</td>\n",
       "      <td>-2.261857</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>0.507757</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-0.631418</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>1.965775</td>\n",
       "      <td>-1.232622</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>1.345852</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>0.175121</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>-0.038195</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         v1        v2        v3        v4        v5        v6        v7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         v8        v9       v10       v11       v12       v13       v14  \\\n",
       "0  0.098698  0.363787  0.090794 -0.551600 -0.617801 -0.991390 -0.311169   \n",
       "1  0.085102 -0.255425 -0.166974  1.612727  1.065235  0.489095 -0.143772   \n",
       "2  0.247676 -1.514654  0.207643  0.624501  0.066084  0.717293 -0.165946   \n",
       "3  0.377436 -1.387024 -0.054952 -0.226487  0.178228  0.507757 -0.287924   \n",
       "4 -0.270533  0.817739  0.753074 -0.822843  0.538196  1.345852 -1.119670   \n",
       "\n",
       "        v15       v16       v17       v18       v19       v20       v21  \\\n",
       "0  1.468177 -0.470401  0.207971  0.025791  0.403993  0.251412 -0.018307   \n",
       "1  0.635558  0.463917 -0.114805 -0.183361 -0.145783 -0.069083 -0.225775   \n",
       "2  2.345865 -2.890083  1.109969 -0.121359 -2.261857  0.524980  0.247998   \n",
       "3 -0.631418 -1.059647 -0.684093  1.965775 -1.232622 -0.208038 -0.108300   \n",
       "4  0.175121 -0.451449 -0.237033 -0.038195  0.803487  0.408542 -0.009431   \n",
       "\n",
       "        v22       v23       v24       v25       v26       v27       v28  \\\n",
       "0  0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "1 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
       "2  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "3  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "4  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
       "\n",
       "   amount  class  \n",
       "0  149.62      0  \n",
       "1    2.69      0  \n",
       "2  378.66      0  \n",
       "3  123.50      0  \n",
       "4   69.99      0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================== Shape of dataset ===================\n",
      "(283726, 30)\n",
      "=================== Data types in dataset ===================\n",
      "v1        float64\n",
      "v2        float64\n",
      "v3        float64\n",
      "v4        float64\n",
      "v5        float64\n",
      "v6        float64\n",
      "v7        float64\n",
      "v8        float64\n",
      "v9        float64\n",
      "v10       float64\n",
      "v11       float64\n",
      "v12       float64\n",
      "v13       float64\n",
      "v14       float64\n",
      "v15       float64\n",
      "v16       float64\n",
      "v17       float64\n",
      "v18       float64\n",
      "v19       float64\n",
      "v20       float64\n",
      "v21       float64\n",
      "v22       float64\n",
      "v23       float64\n",
      "v24       float64\n",
      "v25       float64\n",
      "v26       float64\n",
      "v27       float64\n",
      "v28       float64\n",
      "amount    float64\n",
      "class       int64\n",
      "dtype: object\n",
      "=================== Description of data in dataset ===================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>v10</th>\n",
       "      <th>v11</th>\n",
       "      <th>v12</th>\n",
       "      <th>v13</th>\n",
       "      <th>v14</th>\n",
       "      <th>v15</th>\n",
       "      <th>v16</th>\n",
       "      <th>v17</th>\n",
       "      <th>v18</th>\n",
       "      <th>v19</th>\n",
       "      <th>v20</th>\n",
       "      <th>v21</th>\n",
       "      <th>v22</th>\n",
       "      <th>v23</th>\n",
       "      <th>v24</th>\n",
       "      <th>v25</th>\n",
       "      <th>v26</th>\n",
       "      <th>v27</th>\n",
       "      <th>v28</th>\n",
       "      <th>amount</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>283726.000000</td>\n",
       "      <td>283726.000000</td>\n",
       "      <td>283726.000000</td>\n",
       "      <td>283726.000000</td>\n",
       "      <td>283726.000000</td>\n",
       "      <td>283726.000000</td>\n",
       "      <td>283726.000000</td>\n",
       "      <td>283726.000000</td>\n",
       "      <td>283726.000000</td>\n",
       "      <td>283726.000000</td>\n",
       "      <td>283726.000000</td>\n",
       "      <td>283726.000000</td>\n",
       "      <td>283726.000000</td>\n",
       "      <td>283726.000000</td>\n",
       "      <td>283726.000000</td>\n",
       "      <td>283726.000000</td>\n",
       "      <td>283726.000000</td>\n",
       "      <td>283726.000000</td>\n",
       "      <td>283726.000000</td>\n",
       "      <td>283726.000000</td>\n",
       "      <td>283726.000000</td>\n",
       "      <td>283726.000000</td>\n",
       "      <td>283726.000000</td>\n",
       "      <td>283726.000000</td>\n",
       "      <td>283726.000000</td>\n",
       "      <td>283726.000000</td>\n",
       "      <td>283726.000000</td>\n",
       "      <td>283726.000000</td>\n",
       "      <td>283726.000000</td>\n",
       "      <td>283726.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.005917</td>\n",
       "      <td>-0.004135</td>\n",
       "      <td>0.001613</td>\n",
       "      <td>-0.002966</td>\n",
       "      <td>0.001828</td>\n",
       "      <td>-0.001139</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>-0.000854</td>\n",
       "      <td>-0.001596</td>\n",
       "      <td>-0.001441</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>-0.000715</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.001043</td>\n",
       "      <td>0.001162</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.001515</td>\n",
       "      <td>-0.000264</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>-0.000371</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>-0.000232</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.001763</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>88.472687</td>\n",
       "      <td>0.001667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.948026</td>\n",
       "      <td>1.646703</td>\n",
       "      <td>1.508682</td>\n",
       "      <td>1.414184</td>\n",
       "      <td>1.377008</td>\n",
       "      <td>1.331931</td>\n",
       "      <td>1.227664</td>\n",
       "      <td>1.179054</td>\n",
       "      <td>1.095492</td>\n",
       "      <td>1.076407</td>\n",
       "      <td>1.018720</td>\n",
       "      <td>0.994674</td>\n",
       "      <td>0.995430</td>\n",
       "      <td>0.952215</td>\n",
       "      <td>0.914894</td>\n",
       "      <td>0.873696</td>\n",
       "      <td>0.842507</td>\n",
       "      <td>0.837378</td>\n",
       "      <td>0.813379</td>\n",
       "      <td>0.769984</td>\n",
       "      <td>0.723909</td>\n",
       "      <td>0.724550</td>\n",
       "      <td>0.623702</td>\n",
       "      <td>0.605627</td>\n",
       "      <td>0.521220</td>\n",
       "      <td>0.482053</td>\n",
       "      <td>0.395744</td>\n",
       "      <td>0.328027</td>\n",
       "      <td>250.399437</td>\n",
       "      <td>0.040796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-56.407510</td>\n",
       "      <td>-72.715728</td>\n",
       "      <td>-48.325589</td>\n",
       "      <td>-5.683171</td>\n",
       "      <td>-113.743307</td>\n",
       "      <td>-26.160506</td>\n",
       "      <td>-43.557242</td>\n",
       "      <td>-73.216718</td>\n",
       "      <td>-13.434066</td>\n",
       "      <td>-24.588262</td>\n",
       "      <td>-4.797473</td>\n",
       "      <td>-18.683715</td>\n",
       "      <td>-5.791881</td>\n",
       "      <td>-19.214325</td>\n",
       "      <td>-4.498945</td>\n",
       "      <td>-14.129855</td>\n",
       "      <td>-25.162799</td>\n",
       "      <td>-9.498746</td>\n",
       "      <td>-7.213527</td>\n",
       "      <td>-54.497720</td>\n",
       "      <td>-34.830382</td>\n",
       "      <td>-10.933144</td>\n",
       "      <td>-44.807735</td>\n",
       "      <td>-2.836627</td>\n",
       "      <td>-10.295397</td>\n",
       "      <td>-2.604551</td>\n",
       "      <td>-22.565679</td>\n",
       "      <td>-15.430084</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.915951</td>\n",
       "      <td>-0.600321</td>\n",
       "      <td>-0.889682</td>\n",
       "      <td>-0.850134</td>\n",
       "      <td>-0.689830</td>\n",
       "      <td>-0.769031</td>\n",
       "      <td>-0.552509</td>\n",
       "      <td>-0.208828</td>\n",
       "      <td>-0.644221</td>\n",
       "      <td>-0.535578</td>\n",
       "      <td>-0.761649</td>\n",
       "      <td>-0.406198</td>\n",
       "      <td>-0.647862</td>\n",
       "      <td>-0.425732</td>\n",
       "      <td>-0.581452</td>\n",
       "      <td>-0.466860</td>\n",
       "      <td>-0.483928</td>\n",
       "      <td>-0.498014</td>\n",
       "      <td>-0.456289</td>\n",
       "      <td>-0.211469</td>\n",
       "      <td>-0.228305</td>\n",
       "      <td>-0.542700</td>\n",
       "      <td>-0.161703</td>\n",
       "      <td>-0.354453</td>\n",
       "      <td>-0.317485</td>\n",
       "      <td>-0.326763</td>\n",
       "      <td>-0.070641</td>\n",
       "      <td>-0.052818</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.020384</td>\n",
       "      <td>0.063949</td>\n",
       "      <td>0.179963</td>\n",
       "      <td>-0.022248</td>\n",
       "      <td>-0.053468</td>\n",
       "      <td>-0.275168</td>\n",
       "      <td>0.040859</td>\n",
       "      <td>0.021898</td>\n",
       "      <td>-0.052596</td>\n",
       "      <td>-0.093237</td>\n",
       "      <td>-0.032306</td>\n",
       "      <td>0.139072</td>\n",
       "      <td>-0.012927</td>\n",
       "      <td>0.050209</td>\n",
       "      <td>0.049299</td>\n",
       "      <td>0.067119</td>\n",
       "      <td>-0.065867</td>\n",
       "      <td>-0.002142</td>\n",
       "      <td>0.003367</td>\n",
       "      <td>-0.062353</td>\n",
       "      <td>-0.029441</td>\n",
       "      <td>0.006675</td>\n",
       "      <td>-0.011159</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.016278</td>\n",
       "      <td>-0.052172</td>\n",
       "      <td>0.001479</td>\n",
       "      <td>0.011288</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.316068</td>\n",
       "      <td>0.800283</td>\n",
       "      <td>1.026960</td>\n",
       "      <td>0.739647</td>\n",
       "      <td>0.612218</td>\n",
       "      <td>0.396792</td>\n",
       "      <td>0.570474</td>\n",
       "      <td>0.325704</td>\n",
       "      <td>0.595977</td>\n",
       "      <td>0.453619</td>\n",
       "      <td>0.739579</td>\n",
       "      <td>0.616976</td>\n",
       "      <td>0.663178</td>\n",
       "      <td>0.492336</td>\n",
       "      <td>0.650104</td>\n",
       "      <td>0.523512</td>\n",
       "      <td>0.398972</td>\n",
       "      <td>0.501956</td>\n",
       "      <td>0.458508</td>\n",
       "      <td>0.133207</td>\n",
       "      <td>0.186194</td>\n",
       "      <td>0.528245</td>\n",
       "      <td>0.147748</td>\n",
       "      <td>0.439738</td>\n",
       "      <td>0.350667</td>\n",
       "      <td>0.240261</td>\n",
       "      <td>0.091208</td>\n",
       "      <td>0.078276</td>\n",
       "      <td>77.510000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.454930</td>\n",
       "      <td>22.057729</td>\n",
       "      <td>9.382558</td>\n",
       "      <td>16.875344</td>\n",
       "      <td>34.801666</td>\n",
       "      <td>73.301626</td>\n",
       "      <td>120.589494</td>\n",
       "      <td>20.007208</td>\n",
       "      <td>15.594995</td>\n",
       "      <td>23.745136</td>\n",
       "      <td>12.018913</td>\n",
       "      <td>7.848392</td>\n",
       "      <td>7.126883</td>\n",
       "      <td>10.526766</td>\n",
       "      <td>8.877742</td>\n",
       "      <td>17.315112</td>\n",
       "      <td>9.253526</td>\n",
       "      <td>5.041069</td>\n",
       "      <td>5.591971</td>\n",
       "      <td>39.420904</td>\n",
       "      <td>27.202839</td>\n",
       "      <td>10.503090</td>\n",
       "      <td>22.528412</td>\n",
       "      <td>4.584549</td>\n",
       "      <td>7.519589</td>\n",
       "      <td>3.517346</td>\n",
       "      <td>31.612198</td>\n",
       "      <td>33.847808</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  v1             v2             v3             v4  \\\n",
       "count  283726.000000  283726.000000  283726.000000  283726.000000   \n",
       "mean        0.005917      -0.004135       0.001613      -0.002966   \n",
       "std         1.948026       1.646703       1.508682       1.414184   \n",
       "min       -56.407510     -72.715728     -48.325589      -5.683171   \n",
       "25%        -0.915951      -0.600321      -0.889682      -0.850134   \n",
       "50%         0.020384       0.063949       0.179963      -0.022248   \n",
       "75%         1.316068       0.800283       1.026960       0.739647   \n",
       "max         2.454930      22.057729       9.382558      16.875344   \n",
       "\n",
       "                  v5             v6             v7             v8  \\\n",
       "count  283726.000000  283726.000000  283726.000000  283726.000000   \n",
       "mean        0.001828      -0.001139       0.001801      -0.000854   \n",
       "std         1.377008       1.331931       1.227664       1.179054   \n",
       "min      -113.743307     -26.160506     -43.557242     -73.216718   \n",
       "25%        -0.689830      -0.769031      -0.552509      -0.208828   \n",
       "50%        -0.053468      -0.275168       0.040859       0.021898   \n",
       "75%         0.612218       0.396792       0.570474       0.325704   \n",
       "max        34.801666      73.301626     120.589494      20.007208   \n",
       "\n",
       "                  v9            v10            v11            v12  \\\n",
       "count  283726.000000  283726.000000  283726.000000  283726.000000   \n",
       "mean       -0.001596      -0.001441       0.000202      -0.000715   \n",
       "std         1.095492       1.076407       1.018720       0.994674   \n",
       "min       -13.434066     -24.588262      -4.797473     -18.683715   \n",
       "25%        -0.644221      -0.535578      -0.761649      -0.406198   \n",
       "50%        -0.052596      -0.093237      -0.032306       0.139072   \n",
       "75%         0.595977       0.453619       0.739579       0.616976   \n",
       "max        15.594995      23.745136      12.018913       7.848392   \n",
       "\n",
       "                 v13            v14            v15            v16  \\\n",
       "count  283726.000000  283726.000000  283726.000000  283726.000000   \n",
       "mean        0.000603       0.000252       0.001043       0.001162   \n",
       "std         0.995430       0.952215       0.914894       0.873696   \n",
       "min        -5.791881     -19.214325      -4.498945     -14.129855   \n",
       "25%        -0.647862      -0.425732      -0.581452      -0.466860   \n",
       "50%        -0.012927       0.050209       0.049299       0.067119   \n",
       "75%         0.663178       0.492336       0.650104       0.523512   \n",
       "max         7.126883      10.526766       8.877742      17.315112   \n",
       "\n",
       "                 v17            v18            v19            v20  \\\n",
       "count  283726.000000  283726.000000  283726.000000  283726.000000   \n",
       "mean        0.000170       0.001515      -0.000264       0.000187   \n",
       "std         0.842507       0.837378       0.813379       0.769984   \n",
       "min       -25.162799      -9.498746      -7.213527     -54.497720   \n",
       "25%        -0.483928      -0.498014      -0.456289      -0.211469   \n",
       "50%        -0.065867      -0.002142       0.003367      -0.062353   \n",
       "75%         0.398972       0.501956       0.458508       0.133207   \n",
       "max         9.253526       5.041069       5.591971      39.420904   \n",
       "\n",
       "                 v21            v22            v23            v24  \\\n",
       "count  283726.000000  283726.000000  283726.000000  283726.000000   \n",
       "mean       -0.000371      -0.000015       0.000198       0.000214   \n",
       "std         0.723909       0.724550       0.623702       0.605627   \n",
       "min       -34.830382     -10.933144     -44.807735      -2.836627   \n",
       "25%        -0.228305      -0.542700      -0.161703      -0.354453   \n",
       "50%        -0.029441       0.006675      -0.011159       0.041016   \n",
       "75%         0.186194       0.528245       0.147748       0.439738   \n",
       "max        27.202839      10.503090      22.528412       4.584549   \n",
       "\n",
       "                 v25            v26            v27            v28  \\\n",
       "count  283726.000000  283726.000000  283726.000000  283726.000000   \n",
       "mean       -0.000232       0.000149       0.001763       0.000547   \n",
       "std         0.521220       0.482053       0.395744       0.328027   \n",
       "min       -10.295397      -2.604551     -22.565679     -15.430084   \n",
       "25%        -0.317485      -0.326763      -0.070641      -0.052818   \n",
       "50%         0.016278      -0.052172       0.001479       0.011288   \n",
       "75%         0.350667       0.240261       0.091208       0.078276   \n",
       "max         7.519589       3.517346      31.612198      33.847808   \n",
       "\n",
       "              amount          class  \n",
       "count  283726.000000  283726.000000  \n",
       "mean       88.472687       0.001667  \n",
       "std       250.399437       0.040796  \n",
       "min         0.000000       0.000000  \n",
       "25%         5.600000       0.000000  \n",
       "50%        22.000000       0.000000  \n",
       "75%        77.510000       0.000000  \n",
       "max     25691.160000       1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read clean data\n",
    "cc = pd.read_csv(\"../data/cc_clean.csv\")\n",
    "print(\"=================== Top 5 rows of dataset ===================\")\n",
    "display(cc.head())\n",
    "print(\"=================== Shape of dataset ===================\")\n",
    "print(cc.shape)\n",
    "print(\"=================== Data types in dataset ===================\")\n",
    "print(cc.dtypes)\n",
    "print(\"=================== Description of data in dataset ===================\")\n",
    "display(cc.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Prep\n",
    "Split data into training set and test set, prior to any scaling or transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features and target variable\n",
    "features = [col for col in cc.columns if col != 'class']\n",
    "X = cc[features]\n",
    "y = cc['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split using default test size of 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,stratify=y,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Data\n",
    "There are several ways to scale the data usking sklearn's methods. \n",
    "\n",
    "1. StandardScaler\n",
    "2. MinMaxScaler\n",
    "3. RobustScaler\n",
    "\n",
    "Based on [this article](https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02), `MinMaxScaler` bounds the data, preserves the shape of the original distribution and doesn't reduce the importance of outliers. `RobustScaler` reduces effects of outliers relative to `MinMaxScaler`. `StandardScaler` if there is a need for a relatively normal distribution.\n",
    "\n",
    "Since I have already removed outliers and do not need to bound the data to a specific range, I will use `StandardScaler`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance Classes\n",
    "The most basic approach to balance classes is random oversampling/undersampling, which randomly duplicates/deletes examples in the minority/majority class, respectively. \n",
    "\n",
    "For this project, I would not consider undersampling as it would mean working on mere hundreds of data when I have hundred thousands of data available.\n",
    "\n",
    "Oversampling techniques from sklearn's imbalanced-learn library :\n",
    "\n",
    "1. Random Oversampling \n",
    "2. Synthetic Minority Oversampling Technique (SMOTE) \n",
    "3. Adaptive Synthetic Sampling Method (ADASYN) \n",
    "\n",
    "Rather than just randomly duplicating minority samples, `SMOTE` synthesizes elements of the minority class, in the vicinity or already existing elements (ie. uses k-nearest-neighbours to create similar but randomly tweaked new observations). From [this article](https://www.datasciencecentral.com/profiles/blogs/handling-imbalanced-data-sets-in-supervised-learning-using-family), `ADASYN` uses a weighted distribution for different minority class samples to decide the number of synthetic samples. This compensates for any skewed distributions. Whereas for `SMOTE`, it generates the same number of synthetic samples for each original minority sample. \n",
    "\n",
    "`ADASYN` appear to be more robust, so I will apply this oversampling technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count before ADASYN:  Counter({0: 212439, 1: 355})\n",
      "Count after ADASYN:  Counter({0: 212439, 1: 212418})\n"
     ]
    }
   ],
   "source": [
    "# apply ADASYN\n",
    "adasyn = ADASYN(sampling_strategy='minority')\n",
    "counter_before = Counter(y_train)\n",
    "print(\"Count before ADASYN: \", counter_before)\n",
    "#fit and resample with ADASYN\n",
    "X_train_ada, y_train_ada = adasyn.fit_resample(X_train,y_train)\n",
    "counter_after = Counter(y_train_ada)\n",
    "print(\"Count after ADASYN: \", counter_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the classes are pretty balanced, so I move forward with modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection\n",
    "Use a pipeline to scale data, balance classes and run the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate classifiers\n",
    "logreg = LogisticRegression()\n",
    "rfc = RandomForestClassifier()\n",
    "svc = SVC()\n",
    "xgbc = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pipeline\n",
    "p1 = Pipeline([('ss', StandardScaler()),\n",
    "              ('logreg', logreg)])\n",
    "p2 = Pipeline([('ss', StandardScaler()),\n",
    "              ('rfc', rfc)])\n",
    "p3 = Pipeline([('ss', StandardScaler()),\n",
    "              ('svc', svc)])\n",
    "p4 = Pipeline([('ss', StandardScaler()),\n",
    "              ('xgbc', xgbc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "params1 = [{'logreg__solver': ['newton-cg','liblinear'],\n",
    "           'logreg__penalty': ['none', 'l1', 'l2', 'elasticnet'],\n",
    "           'logreg__C': [10, 1.0, 0.1, 0.001]}]\n",
    "params2 = [{'rfc__n_estimators': [100, 250, 500], \n",
    "           'rfc__max_depth': [10,20],\n",
    "           'rfc__min_samples_split': [5,10,20]}]\n",
    "params3 = [{'svc__C': [1], \n",
    "           'svc__kernel': ['linear']}] \n",
    "params4 = [{'xgbc__n_estimators': [50, 100, 250, 500],\n",
    "           'xgbc__max_depth': [3,5,10],\n",
    "           'xgbc__learning_rate': [0.05,0.1,0.3]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up gridsearch for each algo\n",
    "gridcvs = {}\n",
    "\n",
    "inner_cv = KFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "for paramgrid, estimator, name in zip((params1,params2,params3,params4),\n",
    "                                     (p1,p2,p3,p4),\n",
    "                                     ('Logistic Regression','Random Forest Classifier', 'Support Vector Classifier', 'XGBoost Classifier')):\n",
    "    gcv = GridSearchCV(estimator = estimator,\n",
    "                      param_grid = paramgrid,\n",
    "                      scoring = 'roc_auc',\n",
    "                      n_jobs=-1,\n",
    "                      cv=inner_cv,\n",
    "                      verbose=0,\n",
    "                      refit=True)\n",
    "    gridcvs[name]=gcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with Logistic Regression\n",
      "Done with Random Forest Classifier\n",
      "Done with Support Vector Classifier\n",
      "[01:36:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[01:59:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:22:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:45:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:08:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Done with XGBoost Classifier\n",
      "+----------------------------------------------------------------+\n",
      "|                 Cross-validated ROC-AUC score                  |\n",
      "+---------------------------+---------------+--------------------+\n",
      "|         Algorithms        | Roc-Auc Score | Standard Deviation |\n",
      "+---------------------------+---------------+--------------------+\n",
      "|    Logistic Regression    |     98.4%     |      +/- 0.0%      |\n",
      "|  Random Forest Classifier |     100.0%    |      +/- 0.0%      |\n",
      "| Support Vector Classifier |     98.3%     |      +/- 0.0%      |\n",
      "|     XGBoost Classifier    |     100.0%    |      +/- 0.0%      |\n",
      "+---------------------------+---------------+--------------------+\n",
      "Wall time: 10h 34min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# score on algos\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "  \n",
    "a = PrettyTable(title=\"Cross-validated ROC-AUC score\", header_style='title', max_table_width=110)\n",
    "a.field_names =[\"Algorithms\", \"ROC-AUC score\", \"Standard Deviation\"]\n",
    "for name, gs_est in sorted(gridcvs.items()):\n",
    "    nested_score = cross_val_score(gs_est,\n",
    "                                  X=X_train_ada,\n",
    "                                  y=y_train_ada,\n",
    "                                  cv=outer_cv,\n",
    "                                  scoring='roc_auc')\n",
    "    a.add_row([name, f'{round(nested_score.mean(),3)*100}%', f'+/- {round(nested_score.std(),3)*100}%'])\n",
    "    print(f'Done with {name}')\n",
    "#print table\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that `Random Forest Classifier` and `XGBoost Classifier` were the best performing with perfect classification. As both did equally well, I decided to move forward with `Random Forest Classifier` as it takes lesser computing power, compared to `XGBoost Classifier`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "In considering the metrics to evaluate the model, I first look at the prediction error (Type 1 or Type 2 error)\n",
    "\n",
    "<u>Type 1 error</u>\n",
    "\n",
    "Type 1 error is the false positive rate (FPR) which is the fraction of false alerts based on model predictions. This is when the model classifies a transaction as fraudulent when it actually is not. \n",
    "\n",
    "Considering the implications of a Type 1 error, the bank would have to cancel credit cards, which means administrative costs. Customers may be dissatisfied knowing thier cards have been cancelled when the card was not tampered with anyway. This may affect the credibility of the bank. \n",
    "\n",
    "<u>Type 2 error</u>\n",
    "\n",
    "Type 2 error is the false negative rate (FNR) which is the fraction of missed detection based on model predictions. This is when the model classifies a fraudulent transaction as genuine.\n",
    "\n",
    "Misclassifying a fraudulent transaction is more dire as there could be much more losses incurred if say the fraudulent transactions continue unnoticed. Not only that, the bank's reputation and credibility would be hurt. Once the amount is lost, it would be very hard to recover. \n",
    "\n",
    "While recall (percentage of actual positives a model correctly identified) is more important than precision (percentage of true positives among all positives that the model identified), we can't completely ignore precision. Due to this, I choose to evaluate the model using the <u>AUC score</u> from ROC-AUC curve. The curve visualises the trade-off between the TPR and FPR and the better the model, the closer the AUC score is to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|       Random Forest Classifier Scores       |\n",
      "+-----------------------------+---------------+\n",
      "|                             | ROC-AUC Score |\n",
      "+-----------------------------+---------------+\n",
      "| Average score over CV folds |     100.0     |\n",
      "|    Training ROC-AUC Score   |     100.0     |\n",
      "|      Test ROC-AUC Score     |     98.286    |\n",
      "+-----------------------------+---------------+\n",
      "=============== Best Params ===============\n",
      "{'rfc__max_depth': 20, 'rfc__min_samples_split': 10, 'rfc__n_estimators': 250}\n"
     ]
    }
   ],
   "source": [
    "# fitting a model to the whole training set using the \"best\" algorithm - cluster 1\n",
    "best_algo = gridcvs['Random Forest Classifier']\n",
    "\n",
    "best_algo.fit(X_train_ada, y_train_ada)\n",
    "train_auc = roc_auc_score(y_true=y_train_ada, y_score=best_algo.predict_proba(X_train_ada)[:,1])\n",
    "test_auc = roc_auc_score(y_true=y_test, y_score=best_algo.predict_proba(X_test)[:,1])\n",
    "\n",
    "b = PrettyTable(title=\"Random Forest Classifier Scores\")\n",
    "b.field_names = [\" \", \"ROC-AUC Score\"]\n",
    "b.add_row([\"Average score over CV folds\", round(100*best_algo.best_score_,3)])\n",
    "b.add_row([\"Training ROC-AUC Score\", round(100*train_auc,3)])\n",
    "b.add_row([\"Test ROC-AUC Score\", round(100*test_auc,3)])\n",
    "print(b)\n",
    "\n",
    "print(\"=============== Best Params ===============\")\n",
    "print(gridcvs['Random Forest Classifier'].best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot ROC-AUC curve\n",
    "preds = best_algo.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, preds)\n",
    "roc_auc = round(auc(fpr,tpr),3)\n",
    "\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.title(\"ROC-AUC curve\", fontsize=12)\n",
    "plt.plot(fpr,tpr, 'b', label = f'AUC = {roc_auc}')\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0,1],[0,1], 'r--')\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running model predictions on the test data, `Random Forest Classifier` achieved an auc score of 98.3%. This means that the model was able to predict correctly 98.3% of the time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "From credit card transactions data with imbalanced fraudulent and genuine transaction classes, I used ADASYN to balance the classes and ran four classification algorithms to build a model, that was able to achieve 98.1% accurate classification of fraudulent transactions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Points\n",
    "* Balanced classes with oversampling or undersampling. To consider oversampling techniques first, because undersampling is a \"waste\" of data collected already.\n",
    "* Oversampling : ROS, SMOTE, ADASYN\n",
    "* If classes remain unbalanced, can consider using other scoring metrics.\n",
    "* Evaluation metrics for classification metric. ROC-AUC seem all encompassing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Improvements\n",
    "* A lot of processing power is needed to run several models along with tuning hyperparameters. Is there a more efficient way to select between models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Hyperparameter tuning for logistic regression:\n",
    "https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/\n",
    "\n",
    "Hyperparameter tuning for SVC:\n",
    "https://medium.com/all-things-ai/in-depth-parameter-tuning-for-svc-758215394769\n",
    "\n",
    "Classification evaluation metrics: https://www.analyticsvidhya.com/blog/2020/10/how-to-choose-evaluation-metrics-for-classification-model/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
